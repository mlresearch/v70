---
title: Decoupled Neural Interfaces using Synthetic Gradients
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/jaderberg17a/jaderberg17a.pdf
url: http://proceedings.mlr.press/v70/jaderberg17a.html
abstract: 'Training directed neural networks typically requires forward-propagating
  data through a computation graph, followed by backpropagating error signal, to produce
  weight updates. All layers, or more generally, modules, of the network are therefore
  locked, in the sense that they must wait for the remainder of the network to execute
  forwards and propagate error backwards before they can be updated. In this work
  we break this constraint by decoupling modules by introducing a model of the future
  computation of the network graph. These models predict what the result of the modelled
  subgraph will produce using only local information. In particular we focus on modelling
  error gradients: by using the modelled <em>synthetic gradient</em> in place of true
  backpropagated error gradients we decouple subgraphs, and can update them independently
  and asynchronously i.e.\ we realise <em>decoupled neural interfaces</em>. We show
  results for feed-forward models, where every layer is trained asynchronously, recurrent
  neural networks (RNNs) where predicting one’s future gradient extends the time over
  which the RNN can effectively model, and also a hierarchical RNN system with ticking
  at different timescales. Finally, we demonstrate that in addition to predicting
  gradients, the same framework can be used to predict inputs, resulting in models
  which are decoupled in both the forward and backwards pass – amounting to independent
  networks which co-learn such that they can be composed into a single functioning
  corporation.'
layout: inproceedings
id: jaderberg17a
tex_title: Decoupled Neural Interfaces using Synthetic Gradients
firstpage: 1627
lastpage: 1635
page: 1627-1635
order: 1627
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Max
  family: Jaderberg
- given: Wojciech Marian
  family: Czarnecki
- given: Simon
  family: Osindero
- given: Oriol
  family: Vinyals
- given: Alex
  family: Graves
- given: David
  family: Silver
- given: Koray
  family: Kavukcuoglu
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/jaderberg17a/jaderberg17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
